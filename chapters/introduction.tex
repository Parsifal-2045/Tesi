\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
The main aim of high energy physics research is to study particles at the most fundamental level in order to discover how they work and interact. In order to gain insight on the inner workings of matter, it is necessary to make particles reach extremely high energies. Many particles accelerators have been built, but the leading one in terms of highest energy reached is currently the Large Hadron Collider (LHC), operated by CERN (Counseil Européen pour la Recherche Nucléaire). In LHC, protons are accelerated to velocities close to the one of the light and are collimated in two beams that are made to collide at specific sites where the detectors are located. CERN is currently home to four main experiments: ALICE, LHCb, CMS and ATLAS, of which the latter two are general purpose experiments looking to improve our knowledge of the Standard Model. When protons collide at such high energies, many other particles are produced and scattered in every direction. Depending on the charge and the energy possessed by each of these particle, they pass through some or all the slices of the detectors and thus can be identified. Since higher energy collision produce more exotic particles with higher energies, these are the most interesting for the cutting-edge research. For this reason, LHC is periodically subject to hardware upgrades that have mainly two aims. First, increase the energy of the colliding beams up to the theoretical limit of the machine of 7 TeV per beam, which translates in collisions with 14 TeV of energy. Secondly, increase the sensibility of the detectors and the number of collisions observed per unit time (luminosity). Every upgrade has the result of producing more particles per event, which means that more and more data needs to be collected and processed extremely quickly. This is why, together with hardware upgrades, the detectors' software must go through regular updates. In particular, we can distinguish two kinds of algorithms used to process the data coming from the detectors: trigger and reconstruction algorithms. The first kind filters the collisions' data in real time selecting and saving only events that satisfy particular conditions that might make them candidates for new physics. Reconstruction algorithms are used to study these potentially interesting events and aim to link every observed particle with its respective producer. In doing so, these algorithms reproduce the chain of decays that have occurred between the actual collision and the detectors. \newline
In this context, the CMS experiment has invested part of its resources to explore the possibilities offered by heterogeneous computing. This means that different parts of the software can run simultaneously on different kinds of devices like CPUs, GPUs or FPGAs. Recently, a push has been made to improve the portability of code between different architectures and back-ends. Since writing completely different code for every device would be extremely inefficient, some abstraction layers have been considered. In every case, the main objective is to produce an executable that can run on many different architectures while maintaining a level of performance as close as possible to the native implementation. \newline
This thesis will discuss the possibilities offered by one of these abstraction layers: SYCL, which is based on the ISO C++ standard. In particular, we will discuss the porting experience of one particular CMS clustering algorithm from CUDA code, designed to run exclusively on NVIDIA GPUs, to SYCL with a focus on the performance and physics analysis. \newline
In Chapter \ref{ch:1} we will introduce the clustering problem and, in particular, CLUE: one algorithm for clustering in high energy physics and the one chosen for this porting experience. 
In Chapter \ref{ch:2} we will present the SYCL standard and its various implementations, going into more details on Data Parallel C++ as the one chosen to carry out the port. Finally, in Chapter \ref{ch:3} we will show the porting results and the performance measurements.
