\chapter*{Conclusions and future work}
\addcontentsline{toc}{chapter}{Conclusions and future work}
The Large Hadron Collider is scheduled to receive a massive hardware upgrade in the next few years. This has the main objective to push the boundaries of high energy physics research by increasing the number of observed collisions per second by a factor of 10 in order to observe rare events and decays more often. Together with the collider, also the experiments' detectors will have to undergo significant upgrades, not only to keep up with the massively increased data rates, but also to renew components and modules deteriorated by the highly radioactive environment where the detectors are. The increased sensibility of the detectors must be accompanied by corresponding software upgrades to process and analyze the collected data with very strict time constraints. This has led CMS to explore the possibilities offered by heterogeneous computing, which would allow to offload part of the computing work to different accelerators, like GPUs or FPGAs, while increasing performance and efficiency leaving the experiment' hardware budget virtually unchanged. This approach would naturally lead to writing a different source code per implementation, thus making the job of maintaining and updating the code impossible, especially on a large scale such that of CMS reconstruction software. However, a solution to this problem comes from compatibility layers. These are code abstraction layers that has the single aim of providing the ability to write a single source code which can than be compiled to produce an executable file that can run on a wide variety of devices and backends. CMS has already identified some of such compatibility layers that might offer promising results in high energy physics: Alpaka and SYCL. Throughout this work, the performance of SYCL has been explored on a particular reconstruction algorithm, CLUE, already in use in CMS reconstruction workflow. In particular, the SYCL implementation backed by Intel, was used to implement first a standalone version of the algorithm and then integrate the same algorithm in a CMSSW-like framework allowing to measure performance in comparison with native implementations and Alpaka: another compatibility layer already used by CMS. The results obtained in this context are extremely promising, showing the ability to write a single source code and produce an executable able to run on CPUs, Intel GPUs and NVIDIA GPUs with good performance results. 

However, there still are some key issues in the SYCL implementation used:
\begin{itemize}
    \item While it is possible to compile for the CUDA backend, it is not yet officially supported by the compiler included in oneAPI. The use of the open source fork of the compiler, LLVM, is fine for testing implementations but lacks the optimization passes and stability guarantees coming from an official implementation which is scheduled for early 2023;
    \item In general, the documentation is still lacking and often forces the programmer to implement standard methods by hand;
    \item It might be possible to improve performance by using device-specific parameters, but at the same time this would decrease the effective portability of the code.
\end{itemize}

Optimization in general is still ongoing, especially when it comes to memory operations and synchronization between the host and the device. In order to further test the performance offered by SYCL, a porting of the aforementioned Patatrack standalone pixel tracking module (Pixeltrack Standalone)~\cite{pixeltrack} is being developed. By porting a more complex application, many more limits of the SYCL standard and its oneAPI are being discovered and reported to Intel, which is assisting the porting. Once Pixeltrack standalone will have been fully ported to SYCL and integrated with the improvements made to the SYCL implementation of the CMS framework, it will be possible to obtain more significant performance measurements and comparisons with the other implementations. The results' comparison will surely be one of the deciding factors to choose which portability layer to rely on during Run4 of HL-LHC.